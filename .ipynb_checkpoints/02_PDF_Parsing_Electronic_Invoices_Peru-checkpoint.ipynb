{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Author\n",
    "<br>\n",
    "Carolina Saavedra - carolinasaavedra01@gmail.com\n",
    "<br>\n",
    "\n",
    "#### 1. Title \n",
    "<br>\n",
    "02_PDF Parsing for Peruvian Electronic Invoices\n",
    "<br>\n",
    "\n",
    "#### 2. Purpose\n",
    "<br>\n",
    "The following code converts encrypted PDF files into text files, allowing them to be subsequently organized and exported into classified Excel files. The following code consists of two parts. The first part converts the first page of a tax folder into information for a text file. The second part organizes the information from the text file into tables for separate exportation.\n",
    "\n",
    "Below is an image showing how this tax folder is presented:\n",
    "\n",
    "![PDF Example](invoice_example_peru.jpg)\n",
    "\n",
    "The text file would look like this:\n",
    "\n",
    "![text_file_example](text_example_Peru.jpg)\n",
    "\n",
    "As you can see, the text file exports the invoice in a disordered manner.For this reason, the code is adapted with different regex functions to extract all the data. The names below are fake. This document does not contain PII \n",
    "\n",
    "<br>\n",
    "\n",
    "#### 2.1. Table with main information\n",
    "\n",
    "| Nombre del Emisor | RUC del Emisor | Nombre del Receptor | RUC del Receptor | Fecha de Emisión | Concepto         | Monto Bruto | Retención | Monto Neto | ID             |\n",
    "|:----------------- |:-------------- |:------------------- |:---------------- |:---------------- |:---------------- |-----------:|---------:|-----------:|:-------------- |\n",
    "| Carlos Pérez      | 123456789      | Asociados S.A.      | 123456789        | 12 de mayo       | DISEÑO DE LOGOS  |      2500  |       00 |       2500 | nombre_archivo |\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 3. Packages to install\n",
    "<br>\n",
    "Before to run the code you must need install 'pip install pdfplumber'; 'pip install pdfquery'; 'pip install fitz'.\n",
    "\n",
    "#### 4. Code Utility\n",
    "\n",
    "\n",
    "1. **Data Collection Efficiency:** Automated extraction of information from PDFs saves significant time and effort compared to manual data entry or extraction. This efficiency allows researchers to focus more on analysis and interpretation rather than spending resources on tedious data collection tasks.\n",
    "\n",
    "\n",
    "2. **Data Standardization:** Extracting information from PDFs using code ensures consistency and standardization in data collection. This helps to minimize errors and discrepancies that may arise from manual data entry, ensuring the accuracy and reliability of the data used for impact evaluation or RCTs.\n",
    "\n",
    "\n",
    "3. **Scalability:** With a code in place for PDF data extraction, the process can be easily scaled up to handle large volumes of documents efficiently. This scalability is particularly valuable in projects with extensive data requirements or when dealing with a large number of documents.\n",
    "\n",
    "\n",
    "4. **Versatility:** The ability to extract information from PDFs allows researchers to incorporate a diverse range of data sources into their impact evaluation projects or RCTs. This versatility enables researchers to access valuable data from various sources, including reports, surveys, and academic papers, enhancing the comprehensiveness and robustness of the analysis.\n",
    "\n",
    "\n",
    "5. **Automation of Data Preprocessing:** Extracting information from PDFs through code facilitates automated preprocessing of data, including cleaning, formatting, and structuring. This automation streamlines the data preparation process, making it easier to perform subsequent analysis tasks such as data merging, transformation, and analysis, thereby accelerating the overall research workflow.\n",
    "\n",
    "\n",
    "In summary, having a code for extracting information from PDFs provides several advantages for impact evaluation projects or RCTs, including efficiency, standardization, scalability, versatility, and automation of data preprocessing. These benefits contribute to improved data quality, streamlined research processes, and enhanced analytical capabilities, ultimately facilitating more robust and insightful evaluations of social programs or interventions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pdfplumber\n",
    "import PyPDF2\n",
    "import re\n",
    "import pdfquery\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2. Directory where the tax folders are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_folder_path = 'C:\\\\Users\\\\CAROLINA\\\\Dropbox\\\\Project_Name\\\\01_PDFs_Analysis\\\\01_PDFs' # Change here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3. Function to find the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All files must have the extension \".pdf\"\n",
    "pdf_files = [file for file in os.listdir(pdf_folder_path) if file.endswith(\".pdf\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4. Loop for convert pfds to text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File where you store the block notes. Change here\n",
    "output_folder = \"C:\\\\Users\\\\CAROLINA\\\\Dropbox\\\\Project_Name\\\\6_PDFs_Analysis\\\\03_Outputs\\\\01_Block_Notes\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    pdf_path = os.path.join(pdf_folder_path, pdf_file)\n",
    "\n",
    "    try:\n",
    "        \n",
    "        with open(pdf_path, 'rb') as pdfFileObj:\n",
    "            \n",
    "            pdfReader = PyPDF2.PdfReader(pdfFileObj)\n",
    "                       \n",
    "            if pdfReader.is_encrypted: # Verify if the PDF is encrypted\n",
    "                \n",
    "                pdfReader.decrypt(\"\")  # Try to decrypted the file\n",
    "                      \n",
    "            pageObj = pdfReader.pages[0] # Select the first page\n",
    "            extracted_text = pageObj.extract_text()\n",
    "\n",
    "            output_path = os.path.join(output_folder, f\"{os.path.splitext(pdf_file)[0]}_output.txt\")\n",
    "           \n",
    "            with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "                output_file.write(extracted_text)\n",
    "\n",
    "    except Exception as e:   \n",
    "        print(f\"Error processing file {pdf_file}: {str(e)}\")  # General exceptions\n",
    "        continue\n",
    "#Non-export folders are folders that are not electronic tax folders or contain other types of content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5. Directory where text files are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directorio_txt = 'C:\\\\Users\\\\CAROLINA\\\\Dropbox\\\\Project_Name\\\\6_PDFs_Analysis\\\\02_Outputs\\\\01_Block_Notes' # Change here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.6. Logic for export tables with <span style=\"color:purple\">MAIN</span> information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_informacion_desde_txt(ruta_txt):\n",
    "    with open(ruta_txt, 'r', encoding='utf-8') as archivo:\n",
    "        contenido = archivo.read()\n",
    "\n",
    "    # Normalizar texto a una línea para evitar saltos y espacios extras\n",
    "    contenido = re.sub(r'\\s+', ' ', contenido)\n",
    "\n",
    "    def extraer_montos_pegados(texto):\n",
    "        texto = texto.replace(',', '').replace(' ', '')\n",
    "        patron = r'(\\d+\\.\\d{2})\\((\\d+\\.\\d{2})\\)(\\d+\\.\\d{2})'\n",
    "        match = re.search(patron, texto)\n",
    "        if match:\n",
    "            return match.group(1), match.group(2), match.group(3)\n",
    "        else:\n",
    "            return \"NA\", \"NA\", \"NA\"\n",
    "\n",
    "    # Variables por defecto\n",
    "    nombre_emisor = \"NA\"\n",
    "    ruc_emisor = \"NA\"\n",
    "    nombre_receptor = \"NA\"\n",
    "    ruc_receptor = \"NA\"\n",
    "    fecha_emision = \"NA\"\n",
    "    concepto = \"NA\"\n",
    "    monto_bruto = \"NA\"\n",
    "    retencion = \"NA\"\n",
    "    monto_neto = \"NA\"\n",
    "    monto_letras = \"NA\"\n",
    "\n",
    "    # Buscar RUC emisor\n",
    "    ruc_emisor_match = re.search(r'RECIBO POR HONORARIOS ELECTRONICO\\s+Nro:(\\d{11})', contenido)\n",
    "    if ruc_emisor_match:\n",
    "        ruc_emisor = ruc_emisor_match.group(1)\n",
    "\n",
    "    # Buscar nombre emisor (entre Nro: y MZA)\n",
    "    nombre_emisor = \"NA\"\n",
    "    nombre_match = re.search(r'Nro:\\d{11}\\s*(.*)', contenido)\n",
    "    if nombre_match:\n",
    "        linea_siguiente = nombre_match.group(1).strip()\n",
    "        nombre_emisor = re.split(r'\\s*MZA', linea_siguiente)[0].strip()\n",
    "        \n",
    "    # Buscar nombre receptor (entre \"Total Neto Recibido:\" y \"RUC\")\n",
    "    nombre_receptor_match = re.search(r'Total Neto Recibido:(.*?)\\s+RUC', contenido)\n",
    "    if nombre_receptor_match:\n",
    "        nombre_receptor = nombre_receptor_match.group(1).strip()\n",
    "\n",
    "    # Buscar RUC receptor\n",
    "    ruc_receptor_match = re.search(r'RUC\\s+(\\d{11})', contenido)\n",
    "    if ruc_receptor_match:\n",
    "        ruc_receptor = ruc_receptor_match.group(1)\n",
    "\n",
    "    # Buscar concepto (entre \"Por concepto de\" y \"- A\" o \"Forma de Pago\" o \"Fecha de emisión\")\n",
    "    concepto_match = re.search(r'SOLES\\s+(.*?)\\s+(?:- A|Forma de Pago:|Fecha de emisión)', contenido, re.IGNORECASE | re.DOTALL)\n",
    "    if concepto_match:\n",
    "        concepto = concepto_match.group(1).strip()\n",
    "\n",
    "\n",
    "    # Buscar montos pegados (bruto)(retención)(neto)\n",
    "    montos_raw_match = re.search(r'(\\d{1,3}(?:[.,]\\d{3})*[.,]\\d{2}\\(\\d{1,3}(?:[.,]\\d{3})*[.,]\\d{2}\\)\\d{1,3}(?:[.,]\\d{3})*[.,]\\d{2})', contenido)\n",
    "    if montos_raw_match:\n",
    "        monto_bruto, retencion, monto_neto = extraer_montos_pegados(montos_raw_match.group(1))\n",
    "\n",
    "    # Buscar fecha emisión (día mes año)\n",
    "    fecha_match = re.search(r'(\\d{2}\\s+\\w+\\s+\\d{4})', contenido)\n",
    "    if fecha_match:\n",
    "        fecha_emision = fecha_match.group(1)\n",
    "\n",
    "    df_resultante = pd.DataFrame({\n",
    "        'Nombre del Emisor': [nombre_emisor],\n",
    "        'RUC del Emisor': [ruc_emisor],\n",
    "        'Nombre del Receptor': [nombre_receptor],\n",
    "        'RUC del Receptor': [ruc_receptor],\n",
    "        'Fecha de Emisión': [fecha_emision],\n",
    "        'Concepto': [concepto],\n",
    "        'Monto Bruto': [monto_bruto],\n",
    "        'Retención': [retencion],\n",
    "        'Monto Neto': [monto_neto]\n",
    "    })\n",
    "\n",
    "    return df_resultante\n",
    "\n",
    "\n",
    "dfs_resultantes = {}\n",
    "\n",
    "for nombre_archivo in os.listdir(directorio_txt):\n",
    "    if nombre_archivo.endswith('_output.txt'):\n",
    "        ruta_completa = os.path.join(directorio_txt, nombre_archivo)\n",
    "        \n",
    "        id_archivo = os.path.splitext(nombre_archivo)[0].replace('_output', '')  # El nombre del archivo sin \"_output\"\n",
    "        \n",
    "        df_resultante = extraer_informacion_desde_txt(ruta_completa)\n",
    "        \n",
    "        # Añadís la columna ID con el valor del id_archivo\n",
    "        df_resultante['ID'] = id_archivo\n",
    "        \n",
    "        dfs_resultantes[id_archivo] = df_resultante\n",
    "\n",
    "# Export the output\n",
    "directorio_exportacion_m = 'C:\\\\Users\\\\CAROLINA\\\\Dropbox\\\\Project_Name\\\\6_PDFs_Analysis\\\\02_Outputs\\\\csv'\n",
    "\n",
    "for id_archivo, df_resultante in dfs_resultantes.items():\n",
    "    nombre_excel = f'{id_archivo}_Resumen.xlsx'\n",
    "    ruta_excel = os.path.join(directorio_exportacion_m, nombre_excel)\n",
    "    df_resultante.to_excel(ruta_excel, index=False)\n",
    "\n",
    "    print(f\"The Excel file for the ID {id_archivo} has been exported to: {ruta_excel}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
